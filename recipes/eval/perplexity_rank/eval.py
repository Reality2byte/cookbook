# Copyright (c) Fireworks AI, Inc. and affiliates.
#
# All Rights Reserved.

# Run as: python eval.py

import sys
from collections import defaultdict
from typing import Dict, List, Tuple

import hydra
import torch
from omegaconf import DictConfig, OmegaConf
from peft.peft_model import PeftModel
from recipes.common.env import init_env
from recipes.common.peft import load_inference_model
from recipes.common.tokenizer import load_tokenizer
from recipes.eval.perplexity_rank.transform import DatasetTransform
from transformers import AutoTokenizer

from datasets import Dataset, load_dataset


def _prepare_data(config: DictConfig, tokenizer: AutoTokenizer) -> Dataset:
    """
    Prepares evaluation dataset.

    Args:
        config: configuration parameters describing the dataset,
        tokenizer: the tokenizer used to generate EOS token.

    Returns:
        loaded dataset.
    """
    path = config.get("path", config.get("path",
                                         config.get("huggingface_name")))
    dataset = load_dataset(
        path,
        revision=config.get("huggingface_revision"),
        split=config.get("split", "train"),
        data_files=config.get("data_files"),
    )
    print(f"loaded dataset {path} of size {len(dataset)}")

    transform = DatasetTransform.create(config.transform, tokenizer)
    dataset = transform(dataset)

    # for i in range(min(len(dataset), 3)):
    #     print(f"sample prompt {i} for dataset {path}:\n{dataset[i]}")

    return dataset


def _patch(config: DictConfig) -> None:
    """
    Applies module patches.

    Args:
        config: the config describing patching behavior.
    """
    if config.model.flash_attention:
        # flash attention may not have been installed
        from recipes.common.llama_patch import \
            replace_llama_attn_with_flash_attn
        replace_llama_attn_with_flash_attn()


def _perplexity(config: DictConfig, tokenizer: AutoTokenizer, model: PeftModel,
                query: str, document: str, device: torch.device) -> float:
    """
    Calculates perplexity of the generated completion.

    Args:
        config: config describing the evaluation task,
        tokenizer: the tokenizer to use for encoding and decoding,
        model: the sequence generation model,
        query: the query to use in the prompt,
        document: the document to include in the prompt,
        device: the device where to run the inference.

    Returns:
        perplexity calculated on the completion.
    """
    document_prompt = config.prompt_template.format(document=document)
    num_document_tokens = len(
        tokenizer(document_prompt, return_tensors="pt")["input_ids"][0])
    text = document_prompt + query
    input_ids = tokenizer(text, return_tensors="pt")["input_ids"].to(device)
    num_target_tokens = len(input_ids[0]) - num_document_tokens
    target_ids = input_ids.clone()
    target_ids[:, :-num_target_tokens] = -100
    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        # loss is calculated using CrossEntropyLoss which averages over valid labels
        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
        # to the left by 1.
        neg_log_likelihood = outputs.loss

    return neg_log_likelihood


def _recall(perplexities: List[float], scores: List[int],
            recall_limits: List[int]) -> Tuple[List[float], List[float]]:
    """
    Computes recalls from perplexities and ground truth scores.

    Args:
        perplexities: the scores generated by the model (lower is better),
        scores: ground truth labels from the dataset (greater is better),
        recall_limits: the boundaries of recall computation - i.e., the
            values of k in recall@k.

    Returns:
        tuple of lists of the same length equal to len(recall_limits):
            - the recall values for each limit,
            - the baseline recall that corresponds to random ordering of
                data samples.
    """
    top_score = max(scores)
    top_indices = [i for i in range(len(scores)) if scores[i] == top_score]
    labels = set(top_indices)

    prediction_ids = [(prediction, i)
                      for i, prediction in enumerate(perplexities)]
    sorted_prediction_ids = sorted(prediction_ids)
    sorted_perplexities = [x[1] for x in sorted_prediction_ids]

    recalls = []
    baseline_recalls = []
    for recall_limit in recall_limits:
        if len(sorted_perplexities) <= recall_limit:
            recalls.append(-1.)
            baseline_recalls.append(-1)
        else:
            intersection = set(
                sorted_perplexities[:recall_limit]).intersection(labels)
            denominator = min(len(labels), recall_limit)
            recalls.append(float(len(intersection)) / denominator)
            n = float(len(perplexities))
            l = float(len(labels))
            baseline_recalls.append((recall_limit / (n / l)) / l)

    return (recalls, baseline_recalls)


def _evaluate(config: DictConfig, tokenizer: AutoTokenizer, model: PeftModel,
              dataset: Dataset, device: torch.device) -> Dict[str, float]:
    """
    Evaluates the data using perplexity scoring.

    Args:
        config: the configuration describing the evaluation program,
        tokenizer: the tokenizer to use for encoding of the instruction and
            decoding of the results,
        model: the model generating responses,
        dataset: the dataset to use for evaluation,
        device: the default device where the eval should run.

    Returns:
        computed evaluation metrics.
    """
    print("evaluating the model")
    i = 0
    sum_recall = defaultdict(float)
    num_recall = defaultdict(int)
    sum_baseline_recall = defaultdict(float)
    while i < len(dataset):
        query = dataset[i]["query"]
        documents = []
        scores = []
        while i < len(dataset) and dataset[i]["query"] == query:
            document = dataset[i]["document"]
            documents.append(document)
            scores.append(dataset[i]["score"])
            i += 1

        perplexities = []
        min_perplexity = sys.maxsize
        for document in documents:
            perplexity = _perplexity(config, tokenizer, model, query, document,
                                     device)
            perplexities.append(perplexity)
            if perplexity < min_perplexity:
                min_perplexity = perplexity

        recalls, baseline_recalls = _recall(perplexities, scores,
                                            config.recall_limits)
        for recall_limit, recall, baseline_recall in zip(
                config.recall_limits, recalls, baseline_recalls):
            if recall >= 0:
                sum_recall[recall_limit] += recall
                num_recall[recall_limit] += 1
                sum_baseline_recall[recall_limit] += baseline_recall

    result = {
        "recall": {},
        "baseline_recall": {},
    }
    for recall_limit in sum_recall:
        result["recall"][recall_limit] = float(
            sum_recall[recall_limit]) / num_recall[recall_limit]
        result["baseline_recall"][recall_limit] = float(
            sum_baseline_recall[recall_limit]) / num_recall[recall_limit]

    return result


@hydra.main(version_base=None,
            config_path="conf",
            config_name="query_document")
def _app(config: DictConfig) -> None:
    """
    Runs the evaluation program.

    Args:
        config_path: the directory sting config files,
        config_name: selected config to run.
    """
    print(f"config: {OmegaConf.to_yaml(config, resolve=True)}")
    _patch(config)
    env = init_env()
    tokenizer = load_tokenizer(config)
    dataset = _prepare_data(config.dataset, tokenizer)
    model = load_inference_model(config, tokenizer, env.device)
    stats = _evaluate(config, tokenizer, model, dataset, env.device)
    print(f"eval stats: {stats}")


if __name__ == "__main__":
    _app()
